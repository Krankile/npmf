{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_loop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krankile/npmf/blob/main/notebooks/training_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "d8F5tl4NL7FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kernel setup"
      ],
      "metadata": {
        "id": "Rwo44VGZLhAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "hKuFzk7aEmB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8793ec28-258f-4f9a-ce55-0e37557956d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "!git clone https://github.com/Krankile/npmf.git"
      ],
      "metadata": {
        "id": "91KPY7q0LUOw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "gUXnibj8YEi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bda02a1-9daa-4912-8fe7-83773f908e26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mankile\u001b[0m (\u001b[33mkrankile\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General setup"
      ],
      "metadata": {
        "id": "CLAlA0htLgMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd npmf && git pull\n",
        "\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from operator import itemgetter\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from npmf.utils.colors import main, main2, main3\n",
        "from npmf.utils.dataset import TimeDeltaDataset\n",
        "from npmf.utils.dtypes import fundamental_types\n",
        "from npmf.utils.eikon import column_mapping\n",
        "from npmf.utils.tests.utils import pickle_df\n",
        "from npmf.utils.wandb import get_dataset, put_dataset\n",
        "from npmf.utils.training import EarlyStop, to_device\n",
        "\n",
        "from numpy.ma.core import outerproduct\n",
        "from pandas.tseries.offsets import BDay, Day\n",
        "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import wandb as wb"
      ],
      "metadata": {
        "id": "1QSlgObXLq1p"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[main, main2, main3, \"black\"])\n",
        "mpl.rcParams['figure.figsize'] = (6, 4)  # (6, 4) is default and used in the paper"
      ],
      "metadata": {
        "id": "hkTjKKLmLvpl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqy02oAvY7GM",
        "outputId": "68313f04-bbb6-4631-be1b-b298a8837fdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(69)"
      ],
      "metadata": {
        "id": "YVFtfDk0pYtd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Neural network class"
      ],
      "metadata": {
        "id": "qM4PhINrGVa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get some data"
      ],
      "metadata": {
        "id": "PGuFgO6jHPWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "stock_df = get_dataset(\"stock-oil-final:latest\", project=\"master-test\")\n",
        "fundamentals_df = get_dataset(\"fundamentals-oil-final:latest\", project=\"master-test\")\n",
        "meta_df = get_dataset(\"meta-oil-final:latest\", project=\"master-test\")\n",
        "macro_df = get_dataset(\"macro-oil-final:latest\", project=\"master-test\")\n",
        "\n",
        "stock_df = stock_df.drop_duplicates(subset=[\"ticker\", \"date\"])"
      ],
      "metadata": {
        "id": "GHmOVxnnHYU_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the loop! (Like Odd-Geir Lademo)"
      ],
      "metadata": {
        "id": "s5zbIHNQHUNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1Y55gFQSi4Baovmi0kUQGhbgGOBTI03E7)\n"
      ],
      "metadata": {
        "id": "tA8fn-daswS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateNetwork(nn.Module):\n",
        "    def __init__(self, lag_len, meta_cont_len, meta_cat_len, macro_len, hidden_dim, out_len, **params):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lags = nn.Sequential(\n",
        "            nn.Linear(lag_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.meta_cont = nn.Sequential(\n",
        "            nn.Linear(meta_cont_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.meta_cat = [nn.Embedding(l, hidden_dim) for l in meta_cat_len]\n",
        "\n",
        "        self.macro = nn.Sequential(\n",
        "            nn.Linear(macro_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.predict = nn.Sequential(\n",
        "            nn.Linear(3*hidden_dim + 9*hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, out_len),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, lags, meta_cont, meta_cat, macro):\n",
        "\n",
        "        lags = self.lags(lags)\n",
        "        meta_cont = self.meta_cont(meta_cont)\n",
        "        meta_cat = torch.cat([emb(meta_cat[:, i]) for i, emb in enumerate(self.meta_cat)], dim=1)\n",
        "        macro = self.macro(macro)\n",
        "\n",
        "        x = torch.cat((lags, meta_cont, meta_cat, macro), dim=1)\n",
        "        x = self.predict(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKqMY62PGZzj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mape_loss(target, y_pred):\n",
        "    mask = ~target.isnan()\n",
        "    denom = mask.sum(dim=1)\n",
        "    target[target != target] = 0\n",
        "    l = ((((y_pred - target).abs() / (target.abs() + 1e-8) * mask)).sum(dim=1) / denom).mean()\n",
        "    return l"
      ],
      "metadata": {
        "id": "tAcXcNVq3jkY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RunParams:\n",
        "    n_reports: int\n",
        "    training_w: int\n",
        "    forecast_w: int\n",
        "    max_epochs: int\n",
        "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
        "\n",
        "    lag_len: int\n",
        "    meta_cont_len: int\n",
        "    meta_cat_len: List[int]\n",
        "    macro_len: int\n",
        "    out_len: int\n",
        "    hidden_dim: int\n",
        "    batch_size: int\n",
        "\n",
        "    patience: int\n",
        "    min_delta: float"
      ],
      "metadata": {
        "id": "DKFFb26068ZG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_era(model, optimizer, data_train, data_val, stopper, losses, device, params: RunParams, pbar):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    postfix = dict()\n",
        "    for epoch in range(params.max_epochs):\n",
        "        epoch_losses = dict(train=[], val=[])\n",
        "        postfix = {**postfix, \"epoch\": epoch}\n",
        "        pbar.set_postfix(postfix)\n",
        "        for run_type, dataloader in {\"train\": data_train, \"val\": data_val}.items():\n",
        "            model.train(run_type == \"train\")\n",
        "            \n",
        "            for stocks_and_fundamentals, meta_cont, meta_cat, macro, target in to_device(dataloader, device):\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(stocks_and_fundamentals, meta_cont, meta_cat, macro)\n",
        "    \n",
        "                loss = params.loss_fn(target, y_pred)\n",
        "                epoch_losses[run_type].append(loss.item())\n",
        "\n",
        "                if run_type == \"train\":\n",
        "                    train_losses.append(loss.item())\n",
        "                    loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "                else:\n",
        "                    val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "            losses[run_type].append(\n",
        "                np.mean(epoch_losses[run_type])\n",
        "            )\n",
        "\n",
        "        postfix = {**postfix, \"train_loss\": np.mean(train_losses), \"val_loss\": np.mean(val_losses)}\n",
        "        pbar.set_postfix(postfix)\n",
        "\n",
        "        if run_type == \"val\" and stopper(epoch_losses[\"val\"]):\n",
        "            losses[\"epoch_lens\"].append(epoch + 1)\n",
        "            break\n",
        "        \n",
        "        postfix = {**postfix, \"triggers\": f\"{stopper.triggers}/{stopper.patience}\"}\n",
        "        pbar.set_postfix(postfix)\n",
        "        \n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "lRHUgmiYTrC-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params: RunParams) -> nn.Module:\n",
        "    cpus = multiprocessing.cpu_count()\n",
        "    cpus = 0\n",
        "\n",
        "    model = MultivariateNetwork(**asdict(params))\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    stopper = EarlyStop(params.patience, params.min_delta)\n",
        "\n",
        "    date_range = pd.date_range(start=\"2000-12-31\", end=\"2018-10-31\", freq=\"M\")\n",
        "    n_ranges = len(date_range)\n",
        "    periods = iter(date_range)\n",
        "    period = next(periods)\n",
        "\n",
        "    losses = dict(train=[], val=[], epoch_lens=[])\n",
        "\n",
        "    val = TimeDeltaDataset(period, params.training_w, params.forecast_w, params.n_reports, stock_df, fundamentals_df, meta_df, macro_df)\n",
        "\n",
        "    pbar = tqdm(periods, total=(n_ranges-1), desc=f\"Period {period.date()}\", leave=True, position=0)\n",
        "    for period in pbar:\n",
        "        pbar.set_description(f\"Period {period.date()}\")\n",
        "        tra = val\n",
        "        # TODO Refactor this class to only require the top-level params once\n",
        "        val = TimeDeltaDataset(period, params.training_w, params.forecast_w, params.n_reports, stock_df, fundamentals_df, meta_df, macro_df)\n",
        "\n",
        "        data_train = DataLoader(tra, params.batch_size, shuffle=True, drop_last=False, num_workers=cpus)\n",
        "        data_val = DataLoader(val, params.batch_size, shuffle=False, num_workers=cpus)\n",
        "\n",
        "        stopper.reset()\n",
        "        train_one_era(\n",
        "            model=model, \n",
        "            optimizer=optimizer, \n",
        "            data_train=data_train, \n",
        "            data_val=data_val, \n",
        "            stopper=stopper,\n",
        "            losses=losses,\n",
        "            device=device, \n",
        "            params=params,\n",
        "            pbar=pbar,\n",
        "        )\n",
        "\n",
        "    return model, losses"
      ],
      "metadata": {
        "id": "qvhDBuuccqfr"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = RunParams(\n",
        "    n_reports=4,\n",
        "    training_w=240,\n",
        "    forecast_w=20,\n",
        "    loss_fn=mape_loss,\n",
        "    lag_len=302,\n",
        "    meta_cont_len=1,\n",
        "    meta_cat_len=np.array([89, 5, 70, 185, 1, 3, 5, 10, 44]) + 1, \n",
        "    macro_len=1920,\n",
        "    out_len=20,\n",
        "    hidden_dim=32,\n",
        "    batch_size=64,\n",
        "\n",
        "    max_epochs=5,\n",
        "\n",
        "    patience=2,\n",
        "    min_delta=0.01,\n",
        ")\n",
        "\n",
        "model, losses = train(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhH6fIJOUpxy",
        "outputId": "a73c4d7f-681a-460b-d3db-52e4a1890a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Period 2009-01-31:  45%|████▍     | 96/214 [04:26<07:11,  3.66s/it, epoch=1, train_loss=0.179, val_loss=0.167, triggers=0/2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y_d4v-kWnrVS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}