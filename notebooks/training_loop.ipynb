{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_loop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krankile/npmf/blob/main/notebooks/training_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "d8F5tl4NL7FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kernel setup"
      ],
      "metadata": {
        "id": "Rwo44VGZLhAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "hKuFzk7aEmB9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "!git clone https://github.com/Krankile/npmf.git"
      ],
      "metadata": {
        "id": "91KPY7q0LUOw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "gUXnibj8YEi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206e4ed8-bea0-47e3-8f55-d40c40a15bc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General setup"
      ],
      "metadata": {
        "id": "CLAlA0htLgMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd npmf && git pull\n",
        "\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import asdict, dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from operator import itemgetter\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from npmf.utils.colors import main, main2, main3\n",
        "from npmf.utils.dataset import TimeDeltaDataset\n",
        "from npmf.utils.dtypes import fundamental_types\n",
        "from npmf.utils.eikon import column_mapping\n",
        "from npmf.utils.tests import pickle_df\n",
        "from npmf.utils.wandb import get_dataset, put_dataset\n",
        "from numpy.ma.core import outerproduct\n",
        "from pandas.tseries.offsets import BDay, Day\n",
        "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import wandb as wb"
      ],
      "metadata": {
        "id": "1QSlgObXLq1p"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[main, main2, main3, \"black\"])\n",
        "mpl.rcParams['figure.figsize'] = (6, 4)  # (6, 4) is default and used in the paper"
      ],
      "metadata": {
        "id": "hkTjKKLmLvpl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqy02oAvY7GM",
        "outputId": "cb8abbbd-056f-4c08-f50a-9527d381191e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(69)"
      ],
      "metadata": {
        "id": "YVFtfDk0pYtd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Neural network class"
      ],
      "metadata": {
        "id": "qM4PhINrGVa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get some data"
      ],
      "metadata": {
        "id": "PGuFgO6jHPWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "stock_df = get_dataset(\"stock-oil-final:latest\", project=\"master-test\")\n",
        "fundamentals_df = get_dataset(\"fundamentals-oil-final:latest\", project=\"master-test\")\n",
        "meta_df = get_dataset(\"meta-oil-final:latest\", project=\"master-test\")\n",
        "macro_df = get_dataset(\"macro-oil-final:latest\", project=\"master-test\")\n",
        "\n",
        "stock_df = stock_df.drop_duplicates(subset=[\"ticker\", \"date\"])"
      ],
      "metadata": {
        "id": "GHmOVxnnHYU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the loop! (Like Odd-Geir Lademo)"
      ],
      "metadata": {
        "id": "s5zbIHNQHUNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1Y55gFQSi4Baovmi0kUQGhbgGOBTI03E7)\n"
      ],
      "metadata": {
        "id": "tA8fn-daswS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateNetwork(nn.Module):\n",
        "    def __init__(self, lag_len, meta_cont_len, meta_cat_len, macro_len, hidden_dim, out_len, **params):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lags = nn.Sequential(\n",
        "            nn.Linear(lag_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.meta_cont = nn.Sequential(\n",
        "            nn.Linear(meta_cont_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.meta_cat = [nn.Embedding(l, hidden_dim) for l in meta_cat_len]\n",
        "\n",
        "        self.macro = nn.Sequential(\n",
        "            nn.Linear(macro_len, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.predict = nn.Sequential(\n",
        "            nn.Linear(3*hidden_dim + 9*hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, out_len),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, lags, meta_cont, meta_cat, macro):\n",
        "\n",
        "        lags = self.lags(lags)\n",
        "        meta_cont = self.meta_cont(meta_cont)\n",
        "        meta_cat = torch.cat([emb(meta_cat[:, i]) for i, emb in enumerate(self.meta_cat)], dim=1)\n",
        "        macro = self.macro(macro)\n",
        "\n",
        "        x = torch.cat((lags, meta_cont, meta_cat, macro), dim=1)\n",
        "        x = self.predict(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKqMY62PGZzj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.Tensor([[1, 2, np.nan], [1, 2, np.nan], [1, 2, np.nan]]).isnan().sum(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwc0984onRS7",
        "outputId": "e2c49033-700d-44d7-a284-25ecdc731440"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mape_loss(target, y_pred):\n",
        "    return ((y_pred - target).abs() / (target.abs() + 1e-8)).nanmean(dim=1).mean(dim=0)"
      ],
      "metadata": {
        "id": "tAcXcNVq3jkY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RunParams:\n",
        "    n_reports: int\n",
        "    training_w: int\n",
        "    forecast_w: int\n",
        "    epochs: int\n",
        "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
        "\n",
        "    lag_len: int\n",
        "    meta_cont_len: int\n",
        "    meta_cat_len: List[int]\n",
        "    macro_len: int\n",
        "    out_len: int\n",
        "    hidden_dim: int\n",
        "    batch_size: int"
      ],
      "metadata": {
        "id": "DKFFb26068ZG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stocks_in_timeframe(\n",
        "    stock_df, stock_dates, scale=True, remove_na=True\n",
        ") -> pd.DataFrame:\n",
        "    out = pd.DataFrame(\n",
        "        data=0, columns=stock_dates, index=stock_df.ticker.unique(), dtype=np.float64\n",
        "    )\n",
        "    stock_df = stock_df.pivot(index=\"ticker\", columns=\"date\", values=\"market_cap\")\n",
        "    out = out.add(stock_df)\n",
        "\n",
        "    # Remove tickers where data missing for ticker in the whole period\n",
        "    out = out.dropna(axis=0, how=\"all\")\n",
        "\n",
        "    if remove_na:\n",
        "        out: pd.DataFrame = out.ffill(axis=1).replace(np.nan, 0)\n",
        "\n",
        "    # Perform MinMaxScaling on the full dataset\n",
        "    if scale:\n",
        "        out = pd.DataFrame(\n",
        "            data=minmax_scale(out.values, axis=1),\n",
        "            index=out.index,\n",
        "            columns=out.columns,\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_historic_dates(current_time, trading_days):\n",
        "    back_in_time_buffer = timedelta(trading_days + trading_days * 5)\n",
        "\n",
        "    return pd.date_range(\n",
        "        start=current_time - back_in_time_buffer, end=current_time, freq=\"B\"\n",
        "    )[-trading_days:]\n",
        "\n",
        "\n",
        "def get_forecast_dates(\n",
        "    current_time: np.datetime64, forecast_window: int\n",
        ") -> pd.DatetimeIndex:\n",
        "    forward_in_time_buffer = timedelta(forecast_window + forecast_window * 5)\n",
        "    return pd.date_range(\n",
        "        start=current_time + timedelta(1),\n",
        "        end=current_time + forward_in_time_buffer,\n",
        "        freq=\"B\",\n",
        "    )[:forecast_window]\n",
        "\n",
        "\n",
        "def _get_last_market_cap(stock_df: pd.DataFrame) -> pd.Series:\n",
        "    return (\n",
        "        stock_df.dropna(subset=[\"market_cap\"])\n",
        "        .drop_duplicates(subset=[\"ticker\"], keep=\"last\")\n",
        "        .set_index(\"ticker\")\n",
        "        .market_cap.squeeze()\n",
        "        .astype(np.float64)\n",
        "    )\n",
        "\n",
        "\n",
        "def _minmax_scale_series(series: pd.Series) -> pd.Series:\n",
        "    return pd.Series(\n",
        "        minmax_scale(series.to_numpy().reshape((-1, 1))).squeeze(),\n",
        "        index=series.index,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_global_local_column(\n",
        "    stock_df: pd.DataFrame,\n",
        ") -> Tuple[pd.Series, pd.Series, pd.Series]:\n",
        "    apple_market_cap = 2.687 * (10**12)  # ish as of may 2022 (USD)\n",
        "\n",
        "    last_market_cap_col = _get_last_market_cap(stock_df)\n",
        "\n",
        "    relative_to_global_market_column: pd.Series = last_market_cap_col / apple_market_cap\n",
        "    relative_to_current_market_column = _minmax_scale_series(last_market_cap_col)\n",
        "\n",
        "    return (\n",
        "        relative_to_global_market_column,\n",
        "        relative_to_current_market_column,\n",
        "        last_market_cap_col,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_fundamental_df(\n",
        "    fundamentals,\n",
        "    legal_fundamental_df,\n",
        "    n_reports,\n",
        "    relative_to_current_market_column,\n",
        "    relative_to_global_market_column,\n",
        "    last_market_cap_col,\n",
        "):\n",
        "    fund_columns = []\n",
        "    for i in range(n_reports):\n",
        "        fund_columns.extend(\n",
        "            legal_fundamental_df.loc[0, \"revenue\":]\n",
        "            .index.to_series()\n",
        "            .map(lambda title: f\"{title}_q=-{n_reports-i}\")\n",
        "        )\n",
        "    columns = [\"global_relative\"] + [\"peers_relative\"] + fund_columns\n",
        "    fundamental_df = pd.DataFrame(\n",
        "        index=legal_fundamental_df.ticker.unique(), columns=columns\n",
        "    )\n",
        "\n",
        "    fundamental_df[\"peers_relative\"] = relative_to_current_market_column.loc[\n",
        "        fundamental_df.index\n",
        "    ]\n",
        "    fundamental_df[\"global_relative\"] = relative_to_global_market_column.loc[\n",
        "        fundamental_df.index\n",
        "    ]\n",
        "\n",
        "    fundamental_df.loc[:, f\"revenue_q={-n_reports}\":\"net_income_p_q=-1\"] = fundamentals\n",
        "    for q in range(n_reports, 0, -1):\n",
        "        fundamental_df.loc[:, f\"revenue_q={-q}\":f\"fcf_q={-q}\"] = fundamental_df.loc[\n",
        "            :, f\"revenue_q={-q}\":f\"fcf_q={-q}\"\n",
        "        ].div(last_market_cap_col, axis=0)\n",
        "        fundamental_df.loc[\n",
        "            :, f\"total_assets_q={-q}\":f\"total_current_liabilities_q={-q}\"\n",
        "        ] = fundamental_df.loc[\n",
        "            :, f\"total_assets_q={-q}\":f\"total_current_liabilities_q={-q}\"\n",
        "        ].div(\n",
        "            fundamental_df.loc[:, f\"total_assets_q={-q}\"], axis=0\n",
        "        )\n",
        "        fundamental_df = fundamental_df.drop(columns=f\"total_assets_q={-q}\")\n",
        "\n",
        "    fundamental_df = fundamental_df.replace(np.nan, 0)\n",
        "\n",
        "    return fundamental_df\n",
        "\n",
        "\n",
        "def get_last_q_fundamentals(fundamental_df, q):\n",
        "    fundamental_df = fundamental_df[~fundamental_df.date.isna()].astype(\n",
        "        fundamental_types\n",
        "    )\n",
        "    tickers = fundamental_df.ticker.unique()\n",
        "\n",
        "    fundamental_df[\"rank\"] = (\n",
        "        fundamental_df.groupby(\"ticker\")\n",
        "        .date.rank(method=\"first\", ascending=False)\n",
        "        .astype(int)\n",
        "    )\n",
        "    fundamental_df = fundamental_df.set_index([\"ticker\", \"rank\"])\n",
        "    fundamental_df = fundamental_df[fundamental_df.index.get_level_values(1) <= 4].loc[\n",
        "        :, \"revenue\":\n",
        "    ]\n",
        "\n",
        "    multidx = pd.MultiIndex.from_product(\n",
        "        [tickers, range(q, 0, -1)], names=[\"ticker\", \"rank\"]\n",
        "    )\n",
        "    funds = pd.DataFrame(\n",
        "        data=0,\n",
        "        index=multidx,\n",
        "        columns=fundamental_df.loc[:, \"revenue\":].columns,\n",
        "        dtype=fundamental_df.dtypes.values,\n",
        "    )\n",
        "\n",
        "    result = funds.add(fundamental_df).sort_index(ascending=[True, False])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_fundamentals(fundamental_df, stock_tickers, current_time, n_reports):\n",
        "    # Only keep fundamentals for where we have stock data\n",
        "    legal_fundamental_df = fundamental_df[\n",
        "        (fundamental_df.announce_date < current_time)\n",
        "        & (fundamental_df.ticker.isin(stock_tickers))\n",
        "        & ~fundamental_df.date.isna()\n",
        "    ]\n",
        "\n",
        "    # Important dimensions\n",
        "    n_companies_with_fundamentals = len(legal_fundamental_df.ticker.unique())\n",
        "    m_fundamentals = legal_fundamental_df.loc[:, \"revenue\":].shape[1]\n",
        "\n",
        "    # Get last q fundamentals and return NA rows if they are still missing\n",
        "    fundamental_df_all_quarters = get_last_q_fundamentals(\n",
        "        legal_fundamental_df, n_reports\n",
        "    )\n",
        "    fundamentals = fundamental_df_all_quarters.to_numpy().reshape(\n",
        "        (n_companies_with_fundamentals, n_reports * m_fundamentals)\n",
        "    )\n",
        "\n",
        "    return fundamentals, legal_fundamental_df\n",
        "\n",
        "\n",
        "def get_meta_df(meta_df: pd.DataFrame, stocks_and_fundamentals: pd.DataFrame):\n",
        "    legal_meta_df: pd.DataFrame = meta_df.set_index(\"ticker\")\n",
        "\n",
        "    # Join meta and stock-fundamentals\n",
        "    legal_meta_df = legal_meta_df.loc[stocks_and_fundamentals.index, :]\n",
        "    legal_meta_df.loc[:, \"exchange_code\":\"state_province_hq\"] = legal_meta_df.loc[\n",
        "        :, \"exchange_code\":\"state_province_hq\"\n",
        "    ].astype(\"category\")\n",
        "    legal_meta_df.loc[:, \"economic_sector\":\"activity\"] = legal_meta_df.loc[\n",
        "        :, \"economic_sector\":\"activity\"\n",
        "    ].astype(\"category\")\n",
        "\n",
        "    meta_cont = legal_meta_df[\"founding_year\"].astype(np.float64)\n",
        "\n",
        "    meta_cont = meta_cont.replace(to_replace=np.nan, value=meta_cont.mean(skipna=True))\n",
        "    meta_cont = (meta_cont / 2000).to_frame()\n",
        "\n",
        "    cat_cols = legal_meta_df.select_dtypes(\"category\").columns\n",
        "    meta_cat = legal_meta_df[cat_cols].apply(lambda col: col.cat.codes) + 1\n",
        "\n",
        "    return meta_cont, meta_cat\n",
        "\n",
        "\n",
        "def normalize_macro(legal_macro_df, macro_df):\n",
        "    df = legal_macro_df.copy()\n",
        "    for column in [c for c in legal_macro_df.columns if (\"_fx\" not in c)]:\n",
        "        df[column] = legal_macro_df[column] / (\n",
        "            int(math.ceil(macro_df[column].max() / 100.0)) * 100\n",
        "        )\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_macro_df(\n",
        "    macro_df: pd.DataFrame, historic_dates: pd.DatetimeIndex\n",
        ") -> pd.DataFrame:\n",
        "    macro_df = macro_df.set_index(\"date\")\n",
        "\n",
        "    legal_macro_df = macro_df.loc[macro_df.index.isin(historic_dates), :]\n",
        "\n",
        "    full_macro_df = pd.DataFrame(\n",
        "        data=legal_macro_df, index=historic_dates, columns=legal_macro_df.columns\n",
        "    ).ffill(axis=0)\n",
        "    full_macro_df = normalize_macro(full_macro_df, macro_df).replace(np.nan, 0)\n",
        "    return full_macro_df\n",
        "\n",
        "\n",
        "def get_forecast(\n",
        "    stock_df: pd.DataFrame,\n",
        "    stocks_and_fundamentals: pd.DataFrame,\n",
        "    forecast_dates: pd.DatetimeIndex,\n",
        "    last_market_cap_col: pd.Series,\n",
        "):\n",
        "\n",
        "    forecasts: pd.DataFrame = stock_df[stock_df.date.isin(forecast_dates)]\n",
        "\n",
        "    forecasts_unnormalized = get_stocks_in_timeframe(\n",
        "        forecasts,\n",
        "        forecast_dates,\n",
        "        scale=False,\n",
        "        remove_na=False,\n",
        "    )\n",
        "    tickers = stocks_and_fundamentals.index.intersection(forecasts_unnormalized.index)\n",
        "    forecasts_unnormalized = forecasts_unnormalized.loc[tickers, :]\n",
        "\n",
        "    # TODO: Check if using the same MinMax-scaler as for training set is better\n",
        "    forecasts_normalized = forecasts_unnormalized.div(last_market_cap_col.loc[tickers], axis=0)\n",
        " \n",
        "    forecasts_normalized = forecasts_normalized.astype(np.float64)\n",
        "\n",
        "    return forecasts_normalized\n",
        "\n",
        "\n",
        "class TimeDeltaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        current_time: pd.Timestamp,\n",
        "        training_window: int,\n",
        "        forecast_window: int,\n",
        "        n_reports: int,\n",
        "        stock_df: pd.DataFrame,\n",
        "        fundamental_df: pd.DataFrame,\n",
        "        meta_df: pd.DataFrame,\n",
        "        macro_df: pd.DataFrame,\n",
        "    ):\n",
        "        # Get the relevant dates for training and forecasting\n",
        "        historic_dates = get_historic_dates(current_time, training_window)\n",
        "        forecast_dates = get_forecast_dates(current_time, forecast_window)\n",
        "\n",
        "        # Get stock df\n",
        "        legal_stock_df = stock_df.copy().loc[stock_df.date.isin(historic_dates), :]\n",
        "        formatted_stocks = get_stocks_in_timeframe(\n",
        "            legal_stock_df, historic_dates, scale=True, remove_na=True\n",
        "        )\n",
        "\n",
        "        # Get relative size information\n",
        "        (\n",
        "            relative_to_global_market_column,\n",
        "            relative_to_current_market_column,\n",
        "            last_market_cap_col,\n",
        "        ) = get_global_local_column(legal_stock_df)\n",
        "\n",
        "        # Get fundamentals df\n",
        "        stock_tickers: np.array = legal_stock_df.ticker.unique()\n",
        "        fundamentals, legal_fundamental_df = get_fundamentals(\n",
        "            fundamental_df, stock_tickers, current_time, n_reports\n",
        "        )\n",
        "        fundamental_df = create_fundamental_df(\n",
        "            fundamentals,\n",
        "            legal_fundamental_df,\n",
        "            n_reports,\n",
        "            relative_to_current_market_column,\n",
        "            relative_to_global_market_column,\n",
        "            last_market_cap_col,\n",
        "        )\n",
        "\n",
        "        # Combine stocks and fundamentals\n",
        "        # TODO: Review the strategy for dealing with nan values\n",
        "        stocks_and_fundamentals = formatted_stocks.join(fundamental_df).replace(\n",
        "            np.nan, 0\n",
        "        )\n",
        "\n",
        "        # Get forecasts\n",
        "        self.forecast = get_forecast(\n",
        "            stock_df, stocks_and_fundamentals, forecast_dates, last_market_cap_col\n",
        "        )\n",
        "        self.stocks_and_fundamentals = stocks_and_fundamentals.loc[self.forecast.index, :]\n",
        "\n",
        "        # Get meta df\n",
        "        self.meta_cont, self.meta_cat = get_meta_df(\n",
        "            meta_df, self.stocks_and_fundamentals\n",
        "        )\n",
        "\n",
        "        # Get macro df\n",
        "        self.macro_df = get_macro_df(macro_df, historic_dates)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.stocks_and_fundamentals.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return (\n",
        "            self.stocks_and_fundamentals.iloc[idx, :].to_numpy(),\n",
        "            self.meta_cont.iloc[idx, :].to_numpy().astype(np.float64),\n",
        "            self.meta_cat.iloc[idx, :].to_numpy(),\n",
        "            self.macro_df.T.to_numpy().ravel(),\n",
        "            self.forecast.iloc[idx, :].to_numpy(),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "_SKjIwu39kpN"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, data_train, data_val, device, params: RunParams, pbar):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    postfix = dict(train_loss=None, val_loss=None, epoch=0)\n",
        "    for epoch in range(params.epochs):\n",
        "        postfix = {**postfix, \"epoch\": epoch}\n",
        "        pbar.set_postfix(postfix)\n",
        "        for run_type, dataloader in {\"train\": data_train, \"val\": data_val}.items():\n",
        "            model.train(run_type == \"train\")\n",
        "            \n",
        "            for stocks_and_fundamentals, meta_cont, meta_cat, macro, target in dataloader:\n",
        "                \n",
        "                stocks_and_fundamentals = stocks_and_fundamentals.to(torch.float).to(device)\n",
        "                meta_cont = meta_cont.to(torch.float).to(device)\n",
        "                meta_cat = meta_cat.to(torch.long).to(device)\n",
        "                macro = macro.to(torch.float).to(device)\n",
        "                target = target.to(torch.float).to(device)\n",
        "\n",
        "                y_pred = model(stocks_and_fundamentals, meta_cont, meta_cat, macro)\n",
        "\n",
        "                print(\"NaNs\")\n",
        "                print(stocks_and_fundamentals.isnan().sum(), meta_cont.isnan().sum(), meta_cat.isnan().sum(), macro.isnan().sum(), y_pred.isnan().sum(), target.isnan().sum())\n",
        "                print(stocks_and_fundamentals.max(), meta_cont.max(), meta_cat.max(), macro.max(), target.max())\n",
        "\n",
        "                loss = params.loss_fn(target, y_pred)\n",
        "\n",
        "                print(target)\n",
        "                print(y_pred)\n",
        "\n",
        "                l = ((y_pred - target).abs() / (target.abs() + 1e-8)).nanmean(dim=1).mean()\n",
        "                print(\"Loss\", l)\n",
        "\n",
        "                if run_type == \"train\":\n",
        "                    train_losses.append(loss.item())\n",
        "                    loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "                else:\n",
        "                    val_losses.append(loss.item())\n",
        "        postfix = {**postfix, \"train_loss\": np.mean(train_losses), \"val_loss\": np.mean(val_losses)}\n",
        "        pbar.set_postfix(postfix)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "lRHUgmiYTrC-"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    params = RunParams(\n",
        "        n_reports=4,\n",
        "        training_w=240,\n",
        "        forecast_w=20,\n",
        "        epochs=5,\n",
        "        loss_fn=mape_loss,\n",
        "        lag_len=302,\n",
        "        meta_cont_len=1,\n",
        "        meta_cat_len=np.array([89, 5, 70, 185, 1, 3, 5, 10, 44]) + 1, \n",
        "        macro_len=1920,\n",
        "        out_len=20,\n",
        "        hidden_dim=32,\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    cpus = multiprocessing.cpu_count()\n",
        "    cpus = 0\n",
        "\n",
        "    model = MultivariateNetwork(**asdict(params))\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    date_range = pd.date_range(start=\"2000-12-31\", end=\"2018-10-31\", freq=\"M\")\n",
        "    n_ranges = len(date_range)\n",
        "    periods = iter(date_range)\n",
        "    tra = None\n",
        "    period = next(periods)\n",
        "\n",
        "    val = TimeDeltaDataset(period, params.training_w, params.forecast_w, params.n_reports, stock_df, fundamentals_df, meta_df, macro_df)\n",
        "\n",
        "    outer = tqdm(periods, total=(n_ranges-1), desc=f\"Period {period.date()}\", leave=True, position=0)\n",
        "\n",
        "    for period in outer:\n",
        "        outer.set_description(f\"Period {period.date()}\")\n",
        "        tra = val\n",
        "        # TODO Refactor this class to only require the top-level params once\n",
        "        val = TimeDeltaDataset(period, params.training_w, params.forecast_w, params.n_reports, stock_df, fundamentals_df, meta_df, macro_df)\n",
        "\n",
        "        tra_loader = DataLoader(tra, params.batch_size, shuffle=True, drop_last=True, num_workers=cpus)\n",
        "        val_loader = DataLoader(val, params.batch_size, shuffle=False, num_workers=cpus)\n",
        "\n",
        "        train(model, optimizer, tra_loader, val_loader, device, params, pbar=outer)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qvhDBuuccqfr",
        "outputId": "86b14257-fb2d-4b8e-85bd-f78f22f57dd4"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Period 2001-01-31:   0%|          | 0/214 [00:00<?, ?it/s, train_loss=None, val_loss=None, epoch=0]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(0) tensor(199)\n",
            "tensor(1216.8677) tensor(1.0075) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan, 1.0012,  ..., 1.0585, 1.0603, 1.0640],\n",
            "        [   nan, 0.9757, 0.9914,  ..., 1.1402, 1.1416, 1.1617],\n",
            "        [   nan, 1.0079, 0.9757,  ..., 0.9331, 0.9586, 0.9681],\n",
            "        ...,\n",
            "        [   nan, 1.0131, 1.0326,  ...,    nan,    nan, 1.5507],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 1.0899, 1.0935, 1.0956],\n",
            "        [   nan, 0.9425, 0.9558,  ..., 0.9912, 1.0044, 1.0221]])\n",
            "tensor([[ 0.1317,  0.0447, -0.0381,  ...,  0.2179, -0.1232,  0.0799],\n",
            "        [ 0.0524, -0.0387, -0.1125,  ...,  0.3429, -0.1894,  0.1721],\n",
            "        [ 0.2152,  0.1556, -0.0535,  ...,  0.2678, -0.1133,  0.0208],\n",
            "        ...,\n",
            "        [-0.0285, -0.0891, -0.0439,  ...,  0.2412, -0.0348,  0.0685],\n",
            "        [-0.0556, -0.0858,  0.0381,  ...,  0.3119, -0.1704,  0.2227],\n",
            "        [-0.0377, -0.0900, -0.0624,  ...,  0.3189, -0.1293,  0.1695]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(1.0046, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(176)\n",
            "tensor(62.9329) tensor(1.0095) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 0.9980, 1.0891,  ...,    nan,    nan, 0.9910],\n",
            "        [   nan, 1.0170, 1.0271,  ..., 0.8867, 0.8897, 0.8476],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 1.2385, 1.1184, 0.9960],\n",
            "        ...,\n",
            "        [   nan, 1.0167, 0.9398,  ..., 0.9120, 0.9198, 0.9216],\n",
            "        [   nan, 0.9675, 1.0143,  ..., 1.4042, 1.6070, 1.6700],\n",
            "        [   nan,    nan,    nan,  ..., 0.6222,    nan,    nan]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(168)\n",
            "tensor(58.3944) tensor(1.0090) tensor(93) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 0.9333, 0.9778,  ..., 1.1333, 1.1222, 1.0944],\n",
            "        [   nan, 1.0275, 1.0004,  ..., 0.9412, 0.9468, 0.9383],\n",
            "        [   nan, 1.0107, 1.0257,  ...,    nan,    nan,    nan],\n",
            "        ...,\n",
            "        [   nan, 0.9107, 0.8610,  ..., 0.8739, 0.8885, 0.8885],\n",
            "        [   nan, 0.9750, 0.9667,  ...,    nan,    nan,    nan],\n",
            "        [   nan,    nan,    nan,  ..., 0.9356, 0.9262, 0.9309]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(194)\n",
            "tensor(8744.1611) tensor(1.0075) tensor(91) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0317, 1.1737,  ..., 1.0756, 1.0209, 1.0521],\n",
            "        [   nan, 0.9846, 0.9387,  ..., 0.9605, 0.9570, 0.9402],\n",
            "        [   nan, 4.2560, 4.3066,  ..., 4.5864, 4.5500, 4.5794],\n",
            "        ...,\n",
            "        [   nan, 0.9026, 0.9758,  ..., 0.8380, 0.7767, 0.7767],\n",
            "        [   nan, 1.0179, 1.0162,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 1.2385, 1.2426, 1.2450]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(83)\n",
            "tensor(37.0132) tensor(1.0065) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan, 0.9667,  ..., 0.9327, 0.9333, 0.9333],\n",
            "        [   nan,    nan, 0.9552,  ..., 0.8840, 0.8844, 0.8757],\n",
            "        [1.0379, 1.0155, 1.0114,  ..., 0.9749, 1.0304, 0.9784],\n",
            "        ...,\n",
            "        [1.0303,    nan, 0.9392,  ..., 1.1642, 1.3372, 1.2459],\n",
            "        [1.0092, 1.0044, 1.0074,  ..., 0.9880, 0.9856, 0.9869],\n",
            "        [1.1703, 1.0853, 1.0769,  ..., 0.8986, 0.8977, 0.8942]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(80)\n",
            "tensor(4231.9893) tensor(1.0085) tensor(82) tensor(0.3349) tensor(nan)\n",
            "tensor([[0.9638, 0.9653, 0.9171,  ..., 0.9826, 1.0055, 1.0044],\n",
            "        [1.0073, 1.0186, 1.0274,  ..., 1.0438, 1.0387, 1.0019],\n",
            "        [0.9952, 1.0034,    nan,  ...,    nan, 0.9687, 0.8955],\n",
            "        ...,\n",
            "        [1.0132, 1.0105, 0.9737,  ..., 0.9368, 0.9375, 0.9500],\n",
            "        [0.9985, 1.0165, 1.0409,  ..., 0.9255, 0.9224, 0.9190],\n",
            "        [1.0000, 0.9998, 1.2426,  ..., 1.4073, 1.4493, 1.4524]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Period 2001-01-31:   0%|          | 0/214 [00:01<?, ?it/s, train_loss=nan, val_loss=nan, epoch=1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1280) tensor(54)\n",
            "tensor(1172.5795) tensor(1.0070) tensor(87) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0099, 1.0252, 1.0309,  ..., 1.0667, 1.0579, 1.0489],\n",
            "        [1.0762, 1.0693, 1.1421,  ..., 1.1862, 1.1033, 1.0562],\n",
            "        [0.9760, 0.9738, 0.9978,  ..., 0.9896, 1.0063, 0.9363],\n",
            "        ...,\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 0.9803, 0.9793, 0.9754],\n",
            "        [   nan, 1.0131, 0.9934,  ..., 1.0526, 1.0526, 0.9539],\n",
            "        [0.9851, 0.9830, 0.9787,  ...,    nan,    nan, 0.9939]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(52)\n",
            "tensor(1437.2019) tensor(1.0095) tensor(91) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0171, 1.0098, 0.9794,  ..., 0.8364, 0.8165, 0.8274],\n",
            "        [0.9930, 0.9601, 0.9178,  ..., 0.6408, 0.6056, 0.5915],\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 1.1437, 0.9793, 1.6257],\n",
            "        ...,\n",
            "        [1.0366, 1.0018, 1.0272,  ..., 1.0195, 1.0250, 1.0535],\n",
            "        [0.9879, 1.1032, 1.0805,  ..., 1.0364, 1.0358, 1.0128],\n",
            "        [1.0031, 1.2801, 1.2702,  ..., 1.2526, 1.9043, 2.4386]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(160) tensor(4)\n",
            "tensor(50.6934) tensor(1.0090) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0101, 1.0018, 1.0284, 1.0256, 1.0539, 1.0946, 1.1205, 1.1273, 1.0865,\n",
            "         1.0954, 1.1014, 1.1474, 1.2120, 1.1811, 1.1434, 1.1347, 1.1548, 1.1628,\n",
            "         1.1347, 1.1705],\n",
            "        [1.0001, 1.0134, 0.9974, 1.0256, 1.0206, 1.0080, 0.9920, 0.9990, 0.9860,\n",
            "         0.9496, 0.9879, 0.9882, 0.9845, 0.9699, 0.9835, 0.9883, 1.0373, 1.0149,\n",
            "         1.0198, 1.0261],\n",
            "        [1.0193, 1.0344, 1.0675, 1.0785, 1.1151, 1.0713, 1.0808, 1.1171, 1.1319,\n",
            "         1.1180, 1.0707, 1.0733,    nan, 1.0669, 1.0524, 1.0486, 1.0512, 1.0922,\n",
            "         1.0890, 1.0640],\n",
            "        [1.0098, 1.0416, 1.0462, 1.0288, 1.0556, 1.0498, 1.0518, 1.1008, 1.0885,\n",
            "         1.0879, 1.0716, 1.1133, 1.1333, 1.1079, 1.1338, 1.1598, 1.1620, 1.1682,\n",
            "         1.1531, 1.1556],\n",
            "        [1.0031, 0.9107, 1.1748, 1.0816, 1.1427, 0.9018, 0.9618, 0.9548, 0.9250,\n",
            "         0.9804, 1.0689, 1.1794, 1.1243, 1.1794, 1.1821, 1.1801, 1.1814, 1.0991,\n",
            "         1.0090, 1.0843],\n",
            "        [0.9716, 0.9920, 1.0331, 1.0841, 1.0759, 1.0925, 1.0908, 1.0824, 1.0980,\n",
            "         1.1236, 1.1050, 1.0855,    nan, 1.1029, 1.1097, 1.0894, 1.0675, 1.0843,\n",
            "         1.0833, 1.0644],\n",
            "        [0.9922, 0.9862, 1.0077, 1.0023, 1.0025, 1.0042, 1.0203, 1.0191, 1.0076,\n",
            "         0.9974, 0.9804, 0.9985,    nan, 0.9958, 0.9925, 1.0007, 0.9923, 0.9958,\n",
            "         0.9780, 0.9574],\n",
            "        [0.9836, 0.9839, 0.9836, 0.9836, 0.9836, 0.9836, 0.9839, 0.9836, 0.9836,\n",
            "         0.9836, 0.9672, 0.9676, 0.9672, 0.9672, 0.9672, 0.9672,    nan, 0.9672,\n",
            "         0.9672, 0.9508]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(167)\n",
            "tensor(8744.1611) tensor(1.0075) tensor(93) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0033, 0.9769,  ..., 0.9978, 1.0209, 1.0000],\n",
            "        [   nan, 0.9062, 0.8802,  ...,    nan,    nan, 1.1771],\n",
            "        [1.0161, 0.9730, 0.9742,  ..., 0.8943, 0.8692,    nan],\n",
            "        ...,\n",
            "        [   nan, 1.0197, 1.0379,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 0.9857, 0.9821,  ..., 0.9110, 0.9711, 0.9805],\n",
            "        [   nan, 1.0079, 0.9872,  ..., 0.9186, 0.9415, 0.9433]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(176)\n",
            "tensor(74.1376) tensor(1.0075) tensor(89) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 0.9518, 0.9398,  ..., 0.8795, 0.8795, 0.8795],\n",
            "        [1.0113, 1.0149, 1.0125,  ..., 0.9914, 0.9491,    nan],\n",
            "        [   nan, 1.0239,    nan,  ..., 0.9941, 1.0285,    nan],\n",
            "        ...,\n",
            "        [1.0143, 1.0039, 1.0286,  ..., 1.0084, 1.0041, 0.9965],\n",
            "        [   nan, 0.9653, 0.9722,  ..., 1.0486, 1.0278, 1.0625],\n",
            "        [   nan, 1.0346,    nan,  ..., 1.0065, 1.0334,    nan]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(188)\n",
            "tensor(60.7358) tensor(1.0090) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0100, 1.0201,  ..., 1.0167, 1.0301, 1.0334],\n",
            "        [   nan, 0.9980, 0.8828,  ...,    nan,    nan, 0.7135],\n",
            "        [   nan, 1.0227, 0.9977,  ..., 0.9445, 0.9875, 0.9672],\n",
            "        ...,\n",
            "        [   nan, 0.9904, 0.9518,  ..., 0.8939, 0.9839, 0.9325],\n",
            "        [   nan, 1.0131, 1.0326,  ...,    nan,    nan, 1.5507],\n",
            "        [   nan,    nan,    nan,  ..., 0.9574, 0.9463, 0.9551]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(189)\n",
            "tensor(175.3771) tensor(1.0095) tensor(89) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 0.8611, 0.8572,  ..., 0.8493, 0.8521, 0.8537],\n",
            "        [   nan, 0.9364, 0.9492,  ..., 0.8051, 0.8432, 0.8390],\n",
            "        [   nan,    nan, 1.0159,  ..., 1.0971, 1.0989, 1.1027],\n",
            "        ...,\n",
            "        [   nan, 1.0000, 1.0137,  ..., 1.0822, 1.0822, 1.1233],\n",
            "        [   nan, 1.0040, 1.0165,  ..., 0.9838, 0.9763, 0.9761],\n",
            "        [   nan,    nan,    nan,  ..., 1.2475, 1.1984, 1.1865]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Period 2001-01-31:   0%|          | 0/214 [00:01<?, ?it/s, train_loss=nan, val_loss=nan, epoch=2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0) tensor(1280) tensor(83)\n",
            "tensor(37.0132) tensor(1.0065) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan, 0.9667,  ..., 0.9327, 0.9333, 0.9333],\n",
            "        [   nan,    nan, 0.9552,  ..., 0.8840, 0.8844, 0.8757],\n",
            "        [1.0379, 1.0155, 1.0114,  ..., 0.9749, 1.0304, 0.9784],\n",
            "        ...,\n",
            "        [1.0303,    nan, 0.9392,  ..., 1.1642, 1.3372, 1.2459],\n",
            "        [1.0092, 1.0044, 1.0074,  ..., 0.9880, 0.9856, 0.9869],\n",
            "        [1.1703, 1.0853, 1.0769,  ..., 0.8986, 0.8977, 0.8942]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(80)\n",
            "tensor(4231.9893) tensor(1.0085) tensor(82) tensor(0.3349) tensor(nan)\n",
            "tensor([[0.9638, 0.9653, 0.9171,  ..., 0.9826, 1.0055, 1.0044],\n",
            "        [1.0073, 1.0186, 1.0274,  ..., 1.0438, 1.0387, 1.0019],\n",
            "        [0.9952, 1.0034,    nan,  ...,    nan, 0.9687, 0.8955],\n",
            "        ...,\n",
            "        [1.0132, 1.0105, 0.9737,  ..., 0.9368, 0.9375, 0.9500],\n",
            "        [0.9985, 1.0165, 1.0409,  ..., 0.9255, 0.9224, 0.9190],\n",
            "        [1.0000, 0.9998, 1.2426,  ..., 1.4073, 1.4493, 1.4524]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(54)\n",
            "tensor(1172.5795) tensor(1.0070) tensor(87) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0099, 1.0252, 1.0309,  ..., 1.0667, 1.0579, 1.0489],\n",
            "        [1.0762, 1.0693, 1.1421,  ..., 1.1862, 1.1033, 1.0562],\n",
            "        [0.9760, 0.9738, 0.9978,  ..., 0.9896, 1.0063, 0.9363],\n",
            "        ...,\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 0.9803, 0.9793, 0.9754],\n",
            "        [   nan, 1.0131, 0.9934,  ..., 1.0526, 1.0526, 0.9539],\n",
            "        [0.9851, 0.9830, 0.9787,  ...,    nan,    nan, 0.9939]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(52)\n",
            "tensor(1437.2019) tensor(1.0095) tensor(91) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0171, 1.0098, 0.9794,  ..., 0.8364, 0.8165, 0.8274],\n",
            "        [0.9930, 0.9601, 0.9178,  ..., 0.6408, 0.6056, 0.5915],\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 1.1437, 0.9793, 1.6257],\n",
            "        ...,\n",
            "        [1.0366, 1.0018, 1.0272,  ..., 1.0195, 1.0250, 1.0535],\n",
            "        [0.9879, 1.1032, 1.0805,  ..., 1.0364, 1.0358, 1.0128],\n",
            "        [1.0031, 1.2801, 1.2702,  ..., 1.2526, 1.9043, 2.4386]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(160) tensor(4)\n",
            "tensor(50.6934) tensor(1.0090) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0101, 1.0018, 1.0284, 1.0256, 1.0539, 1.0946, 1.1205, 1.1273, 1.0865,\n",
            "         1.0954, 1.1014, 1.1474, 1.2120, 1.1811, 1.1434, 1.1347, 1.1548, 1.1628,\n",
            "         1.1347, 1.1705],\n",
            "        [1.0001, 1.0134, 0.9974, 1.0256, 1.0206, 1.0080, 0.9920, 0.9990, 0.9860,\n",
            "         0.9496, 0.9879, 0.9882, 0.9845, 0.9699, 0.9835, 0.9883, 1.0373, 1.0149,\n",
            "         1.0198, 1.0261],\n",
            "        [1.0193, 1.0344, 1.0675, 1.0785, 1.1151, 1.0713, 1.0808, 1.1171, 1.1319,\n",
            "         1.1180, 1.0707, 1.0733,    nan, 1.0669, 1.0524, 1.0486, 1.0512, 1.0922,\n",
            "         1.0890, 1.0640],\n",
            "        [1.0098, 1.0416, 1.0462, 1.0288, 1.0556, 1.0498, 1.0518, 1.1008, 1.0885,\n",
            "         1.0879, 1.0716, 1.1133, 1.1333, 1.1079, 1.1338, 1.1598, 1.1620, 1.1682,\n",
            "         1.1531, 1.1556],\n",
            "        [1.0031, 0.9107, 1.1748, 1.0816, 1.1427, 0.9018, 0.9618, 0.9548, 0.9250,\n",
            "         0.9804, 1.0689, 1.1794, 1.1243, 1.1794, 1.1821, 1.1801, 1.1814, 1.0991,\n",
            "         1.0090, 1.0843],\n",
            "        [0.9716, 0.9920, 1.0331, 1.0841, 1.0759, 1.0925, 1.0908, 1.0824, 1.0980,\n",
            "         1.1236, 1.1050, 1.0855,    nan, 1.1029, 1.1097, 1.0894, 1.0675, 1.0843,\n",
            "         1.0833, 1.0644],\n",
            "        [0.9922, 0.9862, 1.0077, 1.0023, 1.0025, 1.0042, 1.0203, 1.0191, 1.0076,\n",
            "         0.9974, 0.9804, 0.9985,    nan, 0.9958, 0.9925, 1.0007, 0.9923, 0.9958,\n",
            "         0.9780, 0.9574],\n",
            "        [0.9836, 0.9839, 0.9836, 0.9836, 0.9836, 0.9836, 0.9839, 0.9836, 0.9836,\n",
            "         0.9836, 0.9672, 0.9676, 0.9672, 0.9672, 0.9672, 0.9672,    nan, 0.9672,\n",
            "         0.9672, 0.9508]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(183)\n",
            "tensor(8744.1611) tensor(1.0080) tensor(89) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0046, 1.0001,  ..., 1.0690, 1.0726, 1.0746],\n",
            "        [   nan, 1.0086, 1.0069,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0278, 0.9823,  ..., 0.9283, 0.9596, 0.9810],\n",
            "        ...,\n",
            "        [   nan, 0.9126, 0.9048,  ...,    nan,    nan, 0.8625],\n",
            "        [   nan, 1.0289, 1.1019,  ..., 1.1605,    nan, 1.1377],\n",
            "        [   nan, 1.0114, 1.0114,  ..., 1.0265, 1.0227, 1.0227]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(172)\n",
            "tensor(44.2860) tensor(1.0075) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.5791, 2.2456,  ..., 3.0035, 3.7947, 4.5848],\n",
            "        [   nan,    nan, 1.0000,  ..., 1.0932, 1.1237, 1.1201],\n",
            "        [   nan, 1.0113, 1.0067,  ..., 0.6649, 0.6671, 1.0026],\n",
            "        ...,\n",
            "        [   nan, 1.1026, 1.0976,  ..., 1.4500, 1.4548, 1.4576],\n",
            "        [   nan,    nan,    nan,  ..., 1.0279, 1.0790, 1.0519],\n",
            "        [   nan, 0.9907, 0.9467,  ..., 0.9666, 0.9641, 0.9955]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(182)\n",
            "tensor(175.3771) tensor(1.0090) tensor(93) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0211, 1.0231,  ..., 1.0111, 1.0419, 1.0480],\n",
            "        [   nan, 1.0135, 1.0284,  ...,    nan,    nan, 1.4636],\n",
            "        [   nan, 0.9778, 0.9758,  ..., 0.9243, 0.9030, 0.8866],\n",
            "        ...,\n",
            "        [   nan, 1.0068, 0.9842,  ..., 0.9842, 0.9865, 0.9797],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 1.4571, 1.4619, 1.4647],\n",
            "        [   nan, 1.0093, 0.9879,  ..., 1.0723, 1.0661, 1.0710]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(201)\n",
            "tensor(68.0944) tensor(1.0095) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0227, 0.9977,  ..., 0.9445, 0.9875, 0.9672],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 0.9908, 0.9941, 0.9960],\n",
            "        [   nan, 0.9897, 1.0103,  ..., 0.9691, 0.9897, 1.0000],\n",
            "        ...,\n",
            "        [   nan, 1.0178, 0.9785,  ..., 0.9460, 0.9852, 0.9785],\n",
            "        [1.0283, 1.0430, 1.0839,  ..., 1.2145, 1.2188,    nan],\n",
            "        [   nan, 0.9107, 0.8610,  ..., 0.8739, 0.8885, 0.8885]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(83)\n",
            "tensor(37.0132) tensor(1.0065) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan, 0.9667,  ..., 0.9327, 0.9333, 0.9333],\n",
            "        [   nan,    nan, 0.9552,  ..., 0.8840, 0.8844, 0.8757],\n",
            "        [1.0379, 1.0155, 1.0114,  ..., 0.9749, 1.0304, 0.9784],\n",
            "        ...,\n",
            "        [1.0303,    nan, 0.9392,  ..., 1.1642, 1.3372, 1.2459],\n",
            "        [1.0092, 1.0044, 1.0074,  ..., 0.9880, 0.9856, 0.9869],\n",
            "        [1.1703, 1.0853, 1.0769,  ..., 0.8986, 0.8977, 0.8942]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(80)\n",
            "tensor(4231.9893) tensor(1.0085) tensor(82) tensor(0.3349) tensor(nan)\n",
            "tensor([[0.9638, 0.9653, 0.9171,  ..., 0.9826, 1.0055, 1.0044],\n",
            "        [1.0073, 1.0186, 1.0274,  ..., 1.0438, 1.0387, 1.0019],\n",
            "        [0.9952, 1.0034,    nan,  ...,    nan, 0.9687, 0.8955],\n",
            "        ...,\n",
            "        [1.0132, 1.0105, 0.9737,  ..., 0.9368, 0.9375, 0.9500],\n",
            "        [0.9985, 1.0165, 1.0409,  ..., 0.9255, 0.9224, 0.9190],\n",
            "        [1.0000, 0.9998, 1.2426,  ..., 1.4073, 1.4493, 1.4524]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(54)\n",
            "tensor(1172.5795) tensor(1.0070) tensor(87) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0099, 1.0252, 1.0309,  ..., 1.0667, 1.0579, 1.0489],\n",
            "        [1.0762, 1.0693, 1.1421,  ..., 1.1862, 1.1033, 1.0562],\n",
            "        [0.9760, 0.9738, 0.9978,  ..., 0.9896, 1.0063, 0.9363],\n",
            "        ...,\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 0.9803, 0.9793, 0.9754],\n",
            "        [   nan, 1.0131, 0.9934,  ..., 1.0526, 1.0526, 0.9539],\n",
            "        [0.9851, 0.9830, 0.9787,  ...,    nan,    nan, 0.9939]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Period 2001-01-31:   0%|          | 0/214 [00:02<?, ?it/s, train_loss=nan, val_loss=nan, epoch=3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(52)\n",
            "tensor(1437.2019) tensor(1.0095) tensor(91) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0171, 1.0098, 0.9794,  ..., 0.8364, 0.8165, 0.8274],\n",
            "        [0.9930, 0.9601, 0.9178,  ..., 0.6408, 0.6056, 0.5915],\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 1.1437, 0.9793, 1.6257],\n",
            "        ...,\n",
            "        [1.0366, 1.0018, 1.0272,  ..., 1.0195, 1.0250, 1.0535],\n",
            "        [0.9879, 1.1032, 1.0805,  ..., 1.0364, 1.0358, 1.0128],\n",
            "        [1.0031, 1.2801, 1.2702,  ..., 1.2526, 1.9043, 2.4386]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(160) tensor(4)\n",
            "tensor(50.6934) tensor(1.0090) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0101, 1.0018, 1.0284, 1.0256, 1.0539, 1.0946, 1.1205, 1.1273, 1.0865,\n",
            "         1.0954, 1.1014, 1.1474, 1.2120, 1.1811, 1.1434, 1.1347, 1.1548, 1.1628,\n",
            "         1.1347, 1.1705],\n",
            "        [1.0001, 1.0134, 0.9974, 1.0256, 1.0206, 1.0080, 0.9920, 0.9990, 0.9860,\n",
            "         0.9496, 0.9879, 0.9882, 0.9845, 0.9699, 0.9835, 0.9883, 1.0373, 1.0149,\n",
            "         1.0198, 1.0261],\n",
            "        [1.0193, 1.0344, 1.0675, 1.0785, 1.1151, 1.0713, 1.0808, 1.1171, 1.1319,\n",
            "         1.1180, 1.0707, 1.0733,    nan, 1.0669, 1.0524, 1.0486, 1.0512, 1.0922,\n",
            "         1.0890, 1.0640],\n",
            "        [1.0098, 1.0416, 1.0462, 1.0288, 1.0556, 1.0498, 1.0518, 1.1008, 1.0885,\n",
            "         1.0879, 1.0716, 1.1133, 1.1333, 1.1079, 1.1338, 1.1598, 1.1620, 1.1682,\n",
            "         1.1531, 1.1556],\n",
            "        [1.0031, 0.9107, 1.1748, 1.0816, 1.1427, 0.9018, 0.9618, 0.9548, 0.9250,\n",
            "         0.9804, 1.0689, 1.1794, 1.1243, 1.1794, 1.1821, 1.1801, 1.1814, 1.0991,\n",
            "         1.0090, 1.0843],\n",
            "        [0.9716, 0.9920, 1.0331, 1.0841, 1.0759, 1.0925, 1.0908, 1.0824, 1.0980,\n",
            "         1.1236, 1.1050, 1.0855,    nan, 1.1029, 1.1097, 1.0894, 1.0675, 1.0843,\n",
            "         1.0833, 1.0644],\n",
            "        [0.9922, 0.9862, 1.0077, 1.0023, 1.0025, 1.0042, 1.0203, 1.0191, 1.0076,\n",
            "         0.9974, 0.9804, 0.9985,    nan, 0.9958, 0.9925, 1.0007, 0.9923, 0.9958,\n",
            "         0.9780, 0.9574],\n",
            "        [0.9836, 0.9839, 0.9836, 0.9836, 0.9836, 0.9836, 0.9839, 0.9836, 0.9836,\n",
            "         0.9836, 0.9672, 0.9676, 0.9672, 0.9672, 0.9672, 0.9672,    nan, 0.9672,\n",
            "         0.9672, 0.9508]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(185)\n",
            "tensor(1216.8677) tensor(1.0095) tensor(93) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 0.9663, 0.9413,  ..., 0.9904, 0.9851, 0.9735],\n",
            "        [   nan,    nan,    nan,  ..., 0.9575, 0.9803, 0.9809],\n",
            "        [   nan, 0.9737, 0.9624,  ..., 0.8604, 0.8610, 0.8661],\n",
            "        ...,\n",
            "        [   nan, 1.0046, 1.0001,  ..., 0.9908, 0.9941, 0.9960],\n",
            "        [   nan, 1.0993, 0.9991,  ..., 0.8152, 1.0039,    nan],\n",
            "        [   nan,    nan,    nan,  ..., 1.2475, 1.1984, 1.1865]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(145)\n",
            "tensor(8744.1611) tensor(1.0075) tensor(89) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan, 1.0046, 1.0001,  ..., 1.8495, 1.8556, 1.8592],\n",
            "        [   nan, 1.0016, 1.0742,  ..., 0.8745, 0.8991,    nan],\n",
            "        [0.9748, 0.9958,    nan,  ..., 0.9775, 1.0111,    nan],\n",
            "        ...,\n",
            "        [   nan, 0.9793, 1.0001,  ..., 0.9991, 0.9774, 0.9876],\n",
            "        [   nan, 1.0524, 1.0604,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0422, 1.0371,  ..., 1.0924, 1.0962, 1.1003]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(168)\n",
            "tensor(50.1496) tensor(1.0085) tensor(92) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan,    nan,  ..., 0.9038, 0.8982, 0.9210],\n",
            "        [   nan, 1.0116, 1.0208,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0163, 1.0163,  ...,    nan,    nan,    nan],\n",
            "        ...,\n",
            "        [   nan, 1.0046, 1.0001,  ..., 0.9248, 0.9278, 0.9296],\n",
            "        [   nan, 0.9319, 0.9379,  ..., 0.8657, 0.8938, 0.9038],\n",
            "        [   nan, 1.0005, 0.9892,  ..., 0.9765, 0.9851, 0.9812]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(233)\n",
            "tensor(57.3923) tensor(1.0075) tensor(91) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0148, 1.0156, 1.0479,  ..., 1.1541, 1.1920,    nan],\n",
            "        [   nan, 1.0046, 1.0001,  ..., 0.6605, 0.6627, 0.6640],\n",
            "        [   nan, 0.9980,    nan,  ...,    nan,    nan, 0.8494],\n",
            "        ...,\n",
            "        [   nan, 1.0086, 1.0069,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0179, 1.0162,  ...,    nan,    nan,    nan],\n",
            "        [   nan, 1.0970, 1.1760,  ...,    nan,    nan, 1.1277]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPeriod 2001-01-31:   0%|          | 0/214 [00:02<?, ?it/s, train_loss=nan, val_loss=nan, epoch=3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(83)\n",
            "tensor(37.0132) tensor(1.0065) tensor(90) tensor(0.3349) tensor(nan)\n",
            "tensor([[   nan,    nan, 0.9667,  ..., 0.9327, 0.9333, 0.9333],\n",
            "        [   nan,    nan, 0.9552,  ..., 0.8840, 0.8844, 0.8757],\n",
            "        [1.0379, 1.0155, 1.0114,  ..., 0.9749, 1.0304, 0.9784],\n",
            "        ...,\n",
            "        [1.0303,    nan, 0.9392,  ..., 1.1642, 1.3372, 1.2459],\n",
            "        [1.0092, 1.0044, 1.0074,  ..., 0.9880, 0.9856, 0.9869],\n",
            "        [1.1703, 1.0853, 1.0769,  ..., 0.8986, 0.8977, 0.8942]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(80)\n",
            "tensor(4231.9893) tensor(1.0085) tensor(82) tensor(0.3349) tensor(nan)\n",
            "tensor([[0.9638, 0.9653, 0.9171,  ..., 0.9826, 1.0055, 1.0044],\n",
            "        [1.0073, 1.0186, 1.0274,  ..., 1.0438, 1.0387, 1.0019],\n",
            "        [0.9952, 1.0034,    nan,  ...,    nan, 0.9687, 0.8955],\n",
            "        ...,\n",
            "        [1.0132, 1.0105, 0.9737,  ..., 0.9368, 0.9375, 0.9500],\n",
            "        [0.9985, 1.0165, 1.0409,  ..., 0.9255, 0.9224, 0.9190],\n",
            "        [1.0000, 0.9998, 1.2426,  ..., 1.4073, 1.4493, 1.4524]])\n",
            "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        ...,\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)\n",
            "Loss tensor(nan, grad_fn=<MeanBackward0>)\n",
            "NaNs\n",
            "tensor(0) tensor(0) tensor(0) tensor(0) tensor(1280) tensor(54)\n",
            "tensor(1172.5795) tensor(1.0070) tensor(87) tensor(0.3349) tensor(nan)\n",
            "tensor([[1.0099, 1.0252, 1.0309,  ..., 1.0667, 1.0579, 1.0489],\n",
            "        [1.0762, 1.0693, 1.1421,  ..., 1.1862, 1.1033, 1.0562],\n",
            "        [0.9760, 0.9738, 0.9978,  ..., 0.9896, 1.0063, 0.9363],\n",
            "        ...,\n",
            "        [1.0031, 1.0018, 0.9941,  ..., 0.9803, 0.9793, 0.9754],\n",
            "        [   nan, 1.0131, 0.9934,  ..., 1.0526, 1.0526, 0.9539],\n",
            "        [0.9851, 0.9830, 0.9787,  ...,    nan,    nan, 0.9939]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-1d8e432b166b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtra_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-209-1d8e432b166b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtra_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-208-583f8b1bc4e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data_train, data_val, device, params, pbar)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    407\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    "
      ],
      "metadata": {
        "id": "Zly64VEmlwGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xIxAVNdcMTY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}